{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "219406cc",
   "metadata": {},
   "source": [
    "# ðŸ—‚ï¸ Batch Processing with TextTools\n",
    "\n",
    "Welcome! ðŸŽ‰ This notebook demonstrates how to use the **Batch Manager** in `texttools`.  \n",
    "\n",
    "The batch manager lets you run large-scale LLM jobs efficiently by splitting data into manageable chunks, processing them in parallel, and saving results safely.\n",
    "\n",
    "Youâ€™ll learn:\n",
    "- How to configure a batch job\n",
    "- How to load and partition input data\n",
    "- How to run, check, and fetch results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f50d56f",
   "metadata": {},
   "source": [
    "## ðŸ”§ Best Practices for Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1562f582",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Best practice for connecting without error ####################\n",
    "\n",
    "# 1- Use a proxy\n",
    "# 2- Run the code on VPS\n",
    "\n",
    "# The first option is better, the data will be locally saved if anything went wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51fefde",
   "metadata": {},
   "source": [
    "## 1. Install & Setup\n",
    "First, make sure you have `texttools` installed:\n",
    "```\n",
    "pip install -U hamta-texttools\n",
    "```\n",
    "\n",
    "Then set your OpenAI (or OpenRouter) API key in a `.env` file:\n",
    "```\n",
    "OPENAI_API_KEY=your_api_key_here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fa9d03",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries\n",
    "Weâ€™ll use `dotenv` for environment variables, `pydantic` for schema validation, and `texttools`â€™ batch manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a2c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from texttools.batch_manager import SimpleBatchManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612ce0d4",
   "metadata": {},
   "source": [
    "## 3. Batch Configuration\n",
    "Here we define limits such as batch size and token budgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d9c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchConfig:\n",
    "    MAX_BATCH_SIZE = 1000\n",
    "    MAX_TOTAL_TOKENS = 2000000\n",
    "    CHARS_PER_TOKEN = 2.7\n",
    "    PROMPT_TOKEN_MULTIPLIER = 1000\n",
    "    BASE_OUTPUT_DIR = \"batch_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9039f89c",
   "metadata": {},
   "source": [
    "## 4. Helper Functions\n",
    "We provide utilities for formatting input data and parsing model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ebf22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_for_batch(data: list[dict[str, Any]]) -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Converts raw data to batch format: [{\"id\": int, \"content\": str}, ...]\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for idx, item in enumerate(data):\n",
    "        if isinstance(item, dict) and \"content\" in item:\n",
    "            result.append({\"id\": item.get(\"id\", idx), \"content\": item[\"content\"]})\n",
    "        elif isinstance(item, str):\n",
    "            result.append({\"id\": idx, \"content\": item})\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid data item at index {idx}: {item}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def parsing_output(\n",
    "    part_idx: int, output_data: list[dict[str, Any]]\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Processes the output from the batch manager. Modify as needed.\n",
    "    \"\"\"\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc027345",
   "metadata": {},
   "source": [
    "## 5. Define Output Schema\n",
    "We use a Pydantic model to enforce structured outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a196378",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputData(BaseModel):\n",
    "    output: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5337b67",
   "metadata": {},
   "source": [
    "## 6. Batch Job Runner\n",
    "This class manages the full lifecycle of a batch job:  \n",
    "- Loading and partitioning data  \n",
    "- Starting and monitoring jobs  \n",
    "- Fetching and saving results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64642fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchJobRunner:\n",
    "    def __init__(\n",
    "        self,\n",
    "        system_prompt: str,\n",
    "        job_name: str,\n",
    "        input_data_path: str,\n",
    "        output_data_path: str,\n",
    "        model: str = \"gpt-4.1-mini\",\n",
    "        output_model=OutputData,\n",
    "    ):\n",
    "        self.config = BatchConfig()\n",
    "        self.system_prompt = system_prompt\n",
    "        self.job_name = job_name\n",
    "        self.input_data_path = input_data_path\n",
    "        self.output_data_path = output_data_path\n",
    "        self.model = model\n",
    "        self.output_model = output_model\n",
    "        self.manager = self._init_manager()\n",
    "        self.data: list[dict[str, Any]] = []\n",
    "        self.parts: list[list[dict[str, Any]]] = []\n",
    "        self._load_data()\n",
    "        self._partition_data()\n",
    "        Path(self.config.BASE_OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _init_manager(self) -> SimpleBatchManager:\n",
    "        load_dotenv()\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        client = OpenAI(api_key=api_key)\n",
    "        return SimpleBatchManager(\n",
    "            client=client,\n",
    "            model=self.model,\n",
    "            prompt_template=self.system_prompt,\n",
    "            output_model=self.output_model,\n",
    "        )\n",
    "\n",
    "    def _load_data(self):\n",
    "        with open(self.input_data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        self.data = data_for_batch(data)\n",
    "\n",
    "    def _partition_data(self):\n",
    "        total_length = sum(len(item[\"content\"]) for item in self.data)\n",
    "        prompt_length = len(self.system_prompt)\n",
    "        total = total_length + (prompt_length * len(self.data))\n",
    "        calculation = total / self.config.CHARS_PER_TOKEN\n",
    "        print(\n",
    "            f\"Total chars: {total_length}, Prompt chars: {prompt_length}, Total: {total}, Tokens: {calculation}\"\n",
    "        )\n",
    "        if calculation < self.config.MAX_TOTAL_TOKENS:\n",
    "            self.parts = [self.data]\n",
    "        else:\n",
    "            # Partition into chunks of MAX_BATCH_SIZE\n",
    "            self.parts = [\n",
    "                self.data[i : i + self.config.MAX_BATCH_SIZE]\n",
    "                for i in range(0, len(self.data), self.config.MAX_BATCH_SIZE)\n",
    "            ]\n",
    "        print(f\"Data split into {len(self.parts)} part(s)\")\n",
    "\n",
    "    def run(self):\n",
    "        for idx, part in enumerate(self.parts):\n",
    "            part_job_name = (\n",
    "                f\"{self.job_name}_part_{idx + 1}\"\n",
    "                if len(self.parts) > 1\n",
    "                else self.job_name\n",
    "            )\n",
    "            print(\n",
    "                f\"\\n--- Processing part {idx + 1}/{len(self.parts)}: {part_job_name} ---\"\n",
    "            )\n",
    "            self._process_part(part, part_job_name, idx)\n",
    "\n",
    "    def _process_part(\n",
    "        self, part: list[dict[str, Any]], part_job_name: str, part_idx: int\n",
    "    ):\n",
    "        while True:\n",
    "            command = (\n",
    "                input(\"Enter command (1.start, 2.check, 3.fetch): \").strip().lower()\n",
    "            )\n",
    "            if command in [\"1\", \"start\"]:\n",
    "                self.manager.start(part, job_name=part_job_name)\n",
    "                print(\"Started batch job.\")\n",
    "                time.sleep(1)\n",
    "            elif command in [\"2\", \"check\"]:\n",
    "                status = self.manager.check_status(job_name=part_job_name)\n",
    "                print(f\"Status: {status}\")\n",
    "                time.sleep(5)\n",
    "                if status == \"completed\":\n",
    "                    print(\"Job completed. You can now fetch results.\")\n",
    "                elif status == \"failed\":\n",
    "                    print(\"Job failed. Clearing state.\")\n",
    "                    self.manager._clear_state(part_job_name)\n",
    "            elif command in [\"3\", \"fetch\"]:\n",
    "                output_data, log = self.manager.fetch_results(\n",
    "                    job_name=part_job_name, save=True, remove_cache=False\n",
    "                )\n",
    "                output_data = parsing_output(part_idx, output_data)\n",
    "                self._save_results(output_data, log, part_idx)\n",
    "                print(\"Fetched and saved results for this part.\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Invalid command. Please enter 1, 2, or 3.\")\n",
    "\n",
    "    def _save_results(\n",
    "        self, output_data: list[dict[str, Any]], log: list[Any], part_idx: int\n",
    "    ):\n",
    "        part_suffix = f\"_part_{part_idx + 1}\" if len(self.parts) > 1 else \"\"\n",
    "        result_path = (\n",
    "            Path(self.config.BASE_OUTPUT_DIR)\n",
    "            / f\"{Path(self.output_data_path).stem}{part_suffix}.json\"\n",
    "        )\n",
    "        with open(result_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(output_data, f, ensure_ascii=False, indent=4)\n",
    "        if log:\n",
    "            log_path = (\n",
    "                Path(self.config.BASE_OUTPUT_DIR)\n",
    "                / f\"{Path(self.output_data_path).stem}{part_suffix}_log.json\"\n",
    "            )\n",
    "            with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(log, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e186fc40",
   "metadata": {},
   "source": [
    "## 7. Running the Job\n",
    "Now we can start the batch job.  \n",
    "Youâ€™ll be prompted for:\n",
    "- System prompt  \n",
    "- Job name  \n",
    "- Input JSON path  \n",
    "- Output JSON path  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d37735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"=== Batch Job Runner ===\")\n",
    "    system_prompt = input(\"Enter system prompt: \").strip()\n",
    "    job_name = input(\"Enter job name: \").strip()\n",
    "    input_data_path = input(\"Enter input data path (JSON): \").strip()\n",
    "    output_data_path = input(\"Enter output data path (JSON): \").strip()\n",
    "    runner = BatchJobRunner(system_prompt, job_name, input_data_path, output_data_path)\n",
    "    runner.run()\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
